{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "file_path = './tuning_results/tune_19/result/tuning_result_tfbind8_lr0.05.csv' \n",
    "df = pd.read_csv(file_path) \n",
    "df = df.sort_values(by='mean (100th)',ascending=False)\n",
    "df = df[df['mean (100th)']>0.95]\n",
    "print(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_fit_samples</th>\n",
       "      <th>sampling_lr</th>\n",
       "      <th>lengthscale</th>\n",
       "      <th>delta</th>\n",
       "      <th>eta</th>\n",
       "      <th>alpha</th>\n",
       "      <th>classifier_free_guidance_weight</th>\n",
       "      <th>mean (100th)</th>\n",
       "      <th>std (100th)</th>\n",
       "      <th>mean (80th)</th>\n",
       "      <th>std (80th)</th>\n",
       "      <th>mean (50th)</th>\n",
       "      <th>std (50th)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.988339</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.820047</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.650288</td>\n",
       "      <td>0.020193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.988339</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.820424</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>0.645745</td>\n",
       "      <td>0.022213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.988339</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.820319</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.642994</td>\n",
       "      <td>0.025989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.988339</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.820006</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.645878</td>\n",
       "      <td>0.025174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.987608</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.840609</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>0.677095</td>\n",
       "      <td>0.020079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.987477</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>0.818428</td>\n",
       "      <td>0.018572</td>\n",
       "      <td>0.647526</td>\n",
       "      <td>0.020534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.987302</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.820393</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.639457</td>\n",
       "      <td>0.024121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.986980</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.838188</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.674140</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.986980</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.839479</td>\n",
       "      <td>0.010970</td>\n",
       "      <td>0.675677</td>\n",
       "      <td>0.018222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986951</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>0.842246</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.663057</td>\n",
       "      <td>0.018548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986766</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.838054</td>\n",
       "      <td>0.017678</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>0.022887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986766</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.842215</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.678477</td>\n",
       "      <td>0.015056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986370</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.846958</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.665152</td>\n",
       "      <td>0.020010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>12000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.668087</td>\n",
       "      <td>0.011865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>12000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986290</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.829430</td>\n",
       "      <td>0.020512</td>\n",
       "      <td>0.667398</td>\n",
       "      <td>0.012750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986259</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.840214</td>\n",
       "      <td>0.013580</td>\n",
       "      <td>0.667523</td>\n",
       "      <td>0.020487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986134</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.844767</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0.668158</td>\n",
       "      <td>0.016175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.986087</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.836565</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>0.679742</td>\n",
       "      <td>0.023850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>12000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986051</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.833201</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.668322</td>\n",
       "      <td>0.012729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>12000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986051</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>0.018017</td>\n",
       "      <td>0.670005</td>\n",
       "      <td>0.012441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.986046</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.842117</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.681923</td>\n",
       "      <td>0.010045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985941</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>0.845438</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.667100</td>\n",
       "      <td>0.016164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985890</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.842615</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.670941</td>\n",
       "      <td>0.022335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985890</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.839260</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.674995</td>\n",
       "      <td>0.026171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985890</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.839390</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.669254</td>\n",
       "      <td>0.022549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985890</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.837887</td>\n",
       "      <td>0.011212</td>\n",
       "      <td>0.666116</td>\n",
       "      <td>0.022953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985890</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.838955</td>\n",
       "      <td>0.014793</td>\n",
       "      <td>0.672805</td>\n",
       "      <td>0.026164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985824</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.841218</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.663887</td>\n",
       "      <td>0.019497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985775</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.840158</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>0.670673</td>\n",
       "      <td>0.019348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985775</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.836551</td>\n",
       "      <td>0.014111</td>\n",
       "      <td>0.672074</td>\n",
       "      <td>0.020393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985762</td>\n",
       "      <td>0.008885</td>\n",
       "      <td>0.843642</td>\n",
       "      <td>0.013492</td>\n",
       "      <td>0.676796</td>\n",
       "      <td>0.018792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985673</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.843774</td>\n",
       "      <td>0.015670</td>\n",
       "      <td>0.668031</td>\n",
       "      <td>0.017552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985673</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.845071</td>\n",
       "      <td>0.013261</td>\n",
       "      <td>0.670652</td>\n",
       "      <td>0.020843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985673</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.846908</td>\n",
       "      <td>0.015194</td>\n",
       "      <td>0.671090</td>\n",
       "      <td>0.017623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985499</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.840165</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.667854</td>\n",
       "      <td>0.016858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985499</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.840847</td>\n",
       "      <td>0.014717</td>\n",
       "      <td>0.659248</td>\n",
       "      <td>0.014744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.985467</td>\n",
       "      <td>0.008239</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.016623</td>\n",
       "      <td>0.632815</td>\n",
       "      <td>0.024758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>13000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.985467</td>\n",
       "      <td>0.008239</td>\n",
       "      <td>0.811405</td>\n",
       "      <td>0.017757</td>\n",
       "      <td>0.626964</td>\n",
       "      <td>0.024360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985448</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.816588</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.642893</td>\n",
       "      <td>0.021402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>17000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985448</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.817905</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.642712</td>\n",
       "      <td>0.019697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985386</td>\n",
       "      <td>0.006178</td>\n",
       "      <td>0.835425</td>\n",
       "      <td>0.011015</td>\n",
       "      <td>0.661541</td>\n",
       "      <td>0.017538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.985386</td>\n",
       "      <td>0.006178</td>\n",
       "      <td>0.835038</td>\n",
       "      <td>0.011026</td>\n",
       "      <td>0.671396</td>\n",
       "      <td>0.013011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985238</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.842467</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.671300</td>\n",
       "      <td>0.012788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985238</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.840793</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.671606</td>\n",
       "      <td>0.014488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.985073</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.840971</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>0.657392</td>\n",
       "      <td>0.015304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_fit_samples  sampling_lr  lengthscale  delta   eta  alpha  \\\n",
       "199            17000         0.05         6.25   0.25  0.75   0.80   \n",
       "195            17000         0.05         6.25   0.25  0.75   0.85   \n",
       "187            17000         0.05         6.25   0.25  0.75   0.95   \n",
       "191            17000         0.05         6.25   0.25  0.75   0.90   \n",
       "86             16000         0.05         6.25   0.25  0.20   0.95   \n",
       "198            17000         0.05         6.25   0.25  0.75   0.80   \n",
       "183            17000         0.05         6.25   0.25  0.75   1.00   \n",
       "135            15000         0.05         6.25   0.25  0.30   0.85   \n",
       "139            15000         0.05         6.25   0.25  0.30   0.80   \n",
       "110            15000         0.05         6.25   0.25  0.25   0.90   \n",
       "82             16000         0.05         6.25   0.25  0.20   1.00   \n",
       "62             16000         0.05         6.25   0.25  0.15   1.00   \n",
       "70             15000         0.05         6.25   0.25  0.15   0.90   \n",
       "130            12000         0.05         6.25   0.25  0.30   0.90   \n",
       "102            12000         0.05         6.25   0.25  0.25   1.00   \n",
       "118            15000         0.05         6.25   0.25  0.25   0.80   \n",
       "30             14000         0.05         6.25   0.25  0.05   0.90   \n",
       "77             16000         0.05         6.25   0.25  0.15   0.80   \n",
       "138            12000         0.05         6.25   0.25  0.30   0.80   \n",
       "134            12000         0.05         6.25   0.25  0.30   0.85   \n",
       "42             16000         0.05         6.25   0.25  0.10   1.00   \n",
       "26             14000         0.05         6.25   0.25  0.05   0.95   \n",
       "119            15000         0.05         6.25   0.25  0.25   0.80   \n",
       "99             15000         0.05         6.25   0.25  0.20   0.80   \n",
       "115            15000         0.05         6.25   0.25  0.25   0.85   \n",
       "111            15000         0.05         6.25   0.25  0.25   0.90   \n",
       "95             15000         0.05         6.25   0.25  0.20   0.85   \n",
       "114            15000         0.05         6.25   0.25  0.25   0.85   \n",
       "107            16000         0.05         6.25   0.25  0.25   0.95   \n",
       "103            16000         0.05         6.25   0.25  0.25   1.00   \n",
       "19             12000         0.05         6.25   0.25  0.00   0.80   \n",
       "98             15000         0.05         6.25   0.25  0.20   0.80   \n",
       "78             15000         0.05         6.25   0.25  0.15   0.80   \n",
       "74             15000         0.05         6.25   0.25  0.15   0.85   \n",
       "86             15000         0.05         6.25   0.25  0.20   0.95   \n",
       "82             15000         0.05         6.25   0.25  0.20   1.00   \n",
       "173            13000         0.05         6.25   0.25  0.50   0.85   \n",
       "169            13000         0.05         6.25   0.25  0.50   0.90   \n",
       "190            17000         0.05         6.25   0.25  0.75   0.90   \n",
       "194            17000         0.05         6.25   0.25  0.75   0.85   \n",
       "127            15000         0.05         6.25   0.25  0.30   0.95   \n",
       "131            15000         0.05         6.25   0.25  0.30   0.90   \n",
       "94             15000         0.05         6.25   0.25  0.20   0.85   \n",
       "90             15000         0.05         6.25   0.25  0.20   0.90   \n",
       "102            15000         0.05         6.25   0.25  0.25   1.00   \n",
       "\n",
       "     classifier_free_guidance_weight  mean (100th)  std (100th)  mean (80th)  \\\n",
       "199                             -1.5      0.988339     0.007687     0.820047   \n",
       "195                             -1.5      0.988339     0.007687     0.820424   \n",
       "187                             -1.5      0.988339     0.007687     0.820319   \n",
       "191                             -1.5      0.988339     0.007687     0.820006   \n",
       "86                              -2.0      0.987608     0.007825     0.840609   \n",
       "198                             -2.0      0.987477     0.007660     0.818428   \n",
       "183                             -1.5      0.987302     0.007340     0.820393   \n",
       "135                             -1.5      0.986980     0.004225     0.838188   \n",
       "139                             -1.5      0.986980     0.004225     0.839479   \n",
       "110                             -2.0      0.986951     0.005840     0.842246   \n",
       "82                              -2.0      0.986766     0.007053     0.838054   \n",
       "62                              -2.0      0.986766     0.007053     0.842215   \n",
       "70                              -2.0      0.986370     0.005431     0.846958   \n",
       "130                             -2.0      0.986290     0.009140     0.828602   \n",
       "102                             -2.0      0.986290     0.009140     0.829430   \n",
       "118                             -2.0      0.986259     0.004256     0.840214   \n",
       "30                              -2.0      0.986134     0.003875     0.844767   \n",
       "77                              -2.5      0.986087     0.007113     0.836565   \n",
       "138                             -2.0      0.986051     0.007006     0.833201   \n",
       "134                             -2.0      0.986051     0.007006     0.832427   \n",
       "42                              -2.0      0.986046     0.007050     0.842117   \n",
       "26                              -2.0      0.985941     0.006420     0.845438   \n",
       "119                             -1.5      0.985890     0.006732     0.842615   \n",
       "99                              -1.5      0.985890     0.006732     0.839260   \n",
       "115                             -1.5      0.985890     0.006732     0.839390   \n",
       "111                             -1.5      0.985890     0.006732     0.837887   \n",
       "95                              -1.5      0.985890     0.006732     0.838955   \n",
       "114                             -2.0      0.985824     0.004416     0.841218   \n",
       "107                             -1.5      0.985775     0.006997     0.840158   \n",
       "103                             -1.5      0.985775     0.006997     0.836551   \n",
       "19                              -1.5      0.985762     0.008885     0.843642   \n",
       "98                              -2.0      0.985673     0.006404     0.843774   \n",
       "78                              -2.0      0.985673     0.006404     0.845071   \n",
       "74                              -2.0      0.985673     0.006404     0.846908   \n",
       "86                              -2.0      0.985499     0.005665     0.840165   \n",
       "82                              -2.0      0.985499     0.005665     0.840847   \n",
       "173                             -2.5      0.985467     0.008239     0.813725   \n",
       "169                             -2.5      0.985467     0.008239     0.811405   \n",
       "190                             -2.0      0.985448     0.008575     0.816588   \n",
       "194                             -2.0      0.985448     0.008575     0.817905   \n",
       "127                             -1.5      0.985386     0.006178     0.835425   \n",
       "131                             -1.5      0.985386     0.006178     0.835038   \n",
       "94                              -2.0      0.985238     0.006472     0.842467   \n",
       "90                              -2.0      0.985238     0.006472     0.840793   \n",
       "102                             -2.0      0.985073     0.006607     0.840971   \n",
       "\n",
       "     std (80th)  mean (50th)  std (50th)  \n",
       "199    0.010891     0.650288    0.020193  \n",
       "195    0.011271     0.645745    0.022213  \n",
       "187    0.012415     0.642994    0.025989  \n",
       "191    0.011750     0.645878    0.025174  \n",
       "86     0.016091     0.677095    0.020079  \n",
       "198    0.018572     0.647526    0.020534  \n",
       "183    0.013013     0.639457    0.024121  \n",
       "135    0.011091     0.674140    0.015100  \n",
       "139    0.010970     0.675677    0.018222  \n",
       "110    0.010827     0.663057    0.018548  \n",
       "82     0.017678     0.676250    0.022887  \n",
       "62     0.012988     0.678477    0.015056  \n",
       "70     0.015271     0.665152    0.020010  \n",
       "130    0.019187     0.668087    0.011865  \n",
       "102    0.020512     0.667398    0.012750  \n",
       "118    0.013580     0.667523    0.020487  \n",
       "30     0.018087     0.668158    0.016175  \n",
       "77     0.017502     0.679742    0.023850  \n",
       "138    0.017571     0.668322    0.012729  \n",
       "134    0.018017     0.670005    0.012441  \n",
       "42     0.018934     0.681923    0.010045  \n",
       "26     0.019917     0.667100    0.016164  \n",
       "119    0.014131     0.670941    0.022335  \n",
       "99     0.014803     0.674995    0.026171  \n",
       "115    0.012183     0.669254    0.022549  \n",
       "111    0.011212     0.666116    0.022953  \n",
       "95     0.014793     0.672805    0.026164  \n",
       "114    0.011859     0.663887    0.019497  \n",
       "107    0.009176     0.670673    0.019348  \n",
       "103    0.014111     0.672074    0.020393  \n",
       "19     0.013492     0.676796    0.018792  \n",
       "98     0.015670     0.668031    0.017552  \n",
       "78     0.013261     0.670652    0.020843  \n",
       "74     0.015194     0.671090    0.017623  \n",
       "86     0.013311     0.667854    0.016858  \n",
       "82     0.014717     0.659248    0.014744  \n",
       "173    0.016623     0.632815    0.024758  \n",
       "169    0.017757     0.626964    0.024360  \n",
       "190    0.019361     0.642893    0.021402  \n",
       "194    0.018244     0.642712    0.019697  \n",
       "127    0.011015     0.661541    0.017538  \n",
       "131    0.011026     0.671396    0.013011  \n",
       "94     0.017657     0.671300    0.012788  \n",
       "90     0.016304     0.671606    0.014488  \n",
       "102    0.013086     0.657392    0.015304  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "num_fit_samples_list = [10000,12000,13000,14000,15000,16000,17000,18000] \n",
    "best_tf8_hyper = None \n",
    "lengthscale = 6.25\n",
    "for num_fit_samples in num_fit_samples_list: \n",
    "    best_tf8_hyper1 =  pd.read_csv(f'tuning_results/tune_23/result/tuning_result_tfbind8_num_fit_samples{num_fit_samples}_lengthscale{lengthscale}_sampling_lr0.05_delta0.25.csv')\n",
    "    best_tf8_hyper1 = best_tf8_hyper1[best_tf8_hyper1['mean (100th)']>0.985]\n",
    "    best_tf8_hyper = pd.concat([best_tf8_hyper,best_tf8_hyper1])\n",
    "best_tf8_hyper_1 = best_tf8_hyper[['eta', 'alpha', 'classifier_free_guidance_weight']].to_numpy()\n",
    "best_tf8_hyper_1 = np.unique(best_tf8_hyper_1,axis=0)\n",
    "print(len(best_tf8_hyper_1))\n",
    "best_tf8_hyper = best_tf8_hyper.sort_values(by= 'mean (100th)',ascending= False)\n",
    "best_tf8_hyper.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "best_tf10_hyper = pd.read_csv('./tuning_results/tune_23/result/tuning_result_tfbind10_lengthscale6.5_sampling_lr0.05_delta0.25.csv')\n",
    "best_tf10_hyper = best_tf10_hyper[best_tf10_hyper['mean (100th)']>0.65]\n",
    "# best_tf8_hyper = None\n",
    "# for num_fit_samples in  [10000,12000,13000,14000,15000,16000,17000,18000] : \n",
    "#     best_tf8_hyper1 = pd.read_csv(f'./tuning_results/tune_23/result/tuning_result_tfbind8_num_fit_samples{num_fit_samples}_lengthscale5.75_sampling_lr0.05_delta0.25.csv')\n",
    "#     best_tf8_hyper1 = best_tf8_hyper1[best_tf8_hyper1['mean (100th)']>0.97] \n",
    "#     best_tf8_hyper = pd.concat([best_tf8_hyper1,best_tf8_hyper],ignore_index=True) \n",
    "# best_tf8_hyper = best_tf8_hyper.merge(best_tf10_hyper, on= ['sampling_lr','lengthscale','delta','eta','alpha','classifier_free_guidance_weight'])\n",
    "# best_tf8_hyper\n",
    "best_tf10_hyper\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "best_tf10_hyper = pd.read_csv('./tuning_results/tune_22_100steps/result/tuning_result_tfbind10_num_fit_samples10000_lengthscale6.0_sampling_lr0.05_delta0.25.csv')\n",
    "best_tf10_hyper = best_tf10_hyper[best_tf10_hyper['mean (100th)']>0.67]\n",
    "best_tf8_hyper  = pd.read_csv('./tuning_results/tune_20/result/tuning_result_tfbind8_lengthscale6.0_sampling_lr0.05_delta0.25.csv')\n",
    "best_tf8_hyper.merge(best_tf10_hyper, on = ['sampling_lr','eta','alpha','delta','lengthscale','classifier_free_guidance_weight'])\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "def set_random_seed(SEED=1234):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_random_seed(0) \n",
    "set_random_seed(0)\n",
    "print(torch.randint(1,10,(10,))) \n",
    "print(1)\n",
    "set_random_seed(1) \n",
    "print(torch.randint(1,10,(10,)))\n",
    "set_random_seed(0) \n",
    "print(torch.randint(1,10,(10,)))\n",
    "set_random_seed(1) \n",
    "print(torch.randint(1,10,(10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create offline_y with duplicate values, and some values repeating in a specific order\n",
    "offline_y = torch.tensor([1.0, 2.0, 3.0, 3.0, 2.0, 1.0, 5.0, 4.0, 5.0, 4.0])\n",
    "offline_x = torch.arange(len(offline_y))  # For simplicity, x is just indices [0, 1, 2, ...]\n",
    "\n",
    "# First code:\n",
    "sorted_indices = torch.argsort(offline_y)[-5:]  # Top 5 largest elements\n",
    "offline_x_1 = offline_x[sorted_indices]\n",
    "offline_y_1 = offline_y[sorted_indices]\n",
    "\n",
    "# Re-sorting the top 5 largest elements\n",
    "indices_1 = torch.argsort(offline_y_1)\n",
    "offline_x_1 = offline_x_1[indices_1]\n",
    "offline_y_1 = offline_y_1[indices_1]\n",
    "\n",
    "print(f\"First Code - offline_x_1: {offline_x_1}, offline_y_1: {offline_y_1}\")\n",
    "\n",
    "# Second code:\n",
    "indices_2 = torch.argsort(offline_y)\n",
    "offline_x_2 = offline_x[indices_2]\n",
    "offline_y_2 = offline_y[indices_2]\n",
    "\n",
    "# Selecting the top 5 largest elements\n",
    "offline_x_2 = offline_x_2[-5:]\n",
    "offline_y_2 = offline_y_2[-5:]\n",
    "\n",
    "print(f\"Second Code - offline_x_2: {offline_x_2}, offline_y_2: {offline_y_2}\")\n",
    "\n",
    "# Check if outputs are identical\n",
    "print(torch.equal(offline_x_1, offline_x_2))  # Check if offline_x are the same\n",
    "print(torch.equal(offline_y_1, offline_y_2))  # Check if offline_y are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_fit_samples</th>\n",
       "      <th>eta_tf</th>\n",
       "      <th>eta_cont</th>\n",
       "      <th>alpha</th>\n",
       "      <th>classifier_free_guidance_weight</th>\n",
       "      <th>mean (100th)_ant</th>\n",
       "      <th>std (100th)_ant</th>\n",
       "      <th>mean (100th)_dkitty</th>\n",
       "      <th>std (100th)_dkitty</th>\n",
       "      <th>mean (100th)_tf8</th>\n",
       "      <th>std (100th)_tf8</th>\n",
       "      <th>mean (100th)_tf10</th>\n",
       "      <th>std (100th)_tf10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.960583</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>0.973676</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.98589</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.685157</td>\n",
       "      <td>0.052762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.973425</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.98589</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.685157</td>\n",
       "      <td>0.052762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.965333</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.972051</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.98589</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.685157</td>\n",
       "      <td>0.052762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_fit_samples  eta_tf  eta_cont  alpha  classifier_free_guidance_weight  \\\n",
       "0            15000     0.2      0.00    0.8                             -1.5   \n",
       "1            15000     0.2      0.05    0.8                             -1.5   \n",
       "2            15000     0.2      0.20    0.8                             -1.5   \n",
       "\n",
       "   mean (100th)_ant  std (100th)_ant  mean (100th)_dkitty  std (100th)_dkitty  \\\n",
       "0          0.960583         0.014716             0.973676            0.004914   \n",
       "1          0.964852         0.015509             0.973425            0.005035   \n",
       "2          0.965333         0.013557             0.972051            0.005118   \n",
       "\n",
       "   mean (100th)_tf8  std (100th)_tf8  mean (100th)_tf10  std (100th)_tf10  \n",
       "0           0.98589         0.006732           0.685157          0.052762  \n",
       "1           0.98589         0.006732           0.685157          0.052762  \n",
       "2           0.98589         0.006732           0.685157          0.052762  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "lengthscale = 6.25\n",
    "best_tf10_hyper = pd.read_csv(f'./tuning_results/tune_23/result/tuning_result_tfbind10_lengthscale{lengthscale}_sampling_lr0.05_delta0.25_1.csv')\n",
    "best_tf10_hyper = best_tf10_hyper[best_tf10_hyper['mean (100th)']>0.67]\n",
    "num_fit_samples_list = [10000,12000,13000,14000,15000,16000,17000,18000] \n",
    "best_tf8_hyper = None \n",
    "print(len(best_tf10_hyper))\n",
    "for num_fit_samples in num_fit_samples_list:\n",
    "    best_tf8_hyper_1 = pd.read_csv(f'./tuning_results/tune_23/result/tuning_result_tfbind8_num_fit_samples{num_fit_samples}_lengthscale{lengthscale}_sampling_lr0.05_delta0.25.csv')\n",
    "    best_tf8_hyper_1 = best_tf8_hyper_1[best_tf8_hyper_1['mean (100th)']>0.985] \n",
    "    best_tf8_hyper = pd.concat([best_tf8_hyper_1, best_tf8_hyper])  \n",
    "print(len(best_tf8_hyper))\n",
    "best_tf8_hyper= best_tf8_hyper.merge(best_tf10_hyper, on = ['sampling_lr','eta','alpha','delta','lengthscale','classifier_free_guidance_weight'])\n",
    "best_tf8_hyper = best_tf8_hyper[['num_fit_samples_x','eta','alpha','classifier_free_guidance_weight','mean (100th)_x','mean (100th)_y','std (100th)_x','std (100th)_y','mean (50th)_x','mean (50th)_y','std (50th)_x','std (50th)_y','mean (80th)_x','mean (80th)_y','std (80th)_x','std (80th)_y']]\n",
    "# Rename columns\n",
    "best_tf8_hyper = best_tf8_hyper.rename(columns={\n",
    "    'num_fit_samples_x': 'num_fit_samples',\n",
    "    'eta': 'eta_tf',\n",
    "    'alpha': 'alpha',\n",
    "    'classifier_free_guidance_weight': 'classifier_free_guidance_weight',\n",
    "    'mean (100th)_x': 'mean (100th)_tf8',\n",
    "    'mean (100th)_y': 'mean (100th)_tf10',\n",
    "    'std (100th)_x': 'std (100th)_tf8',\n",
    "    'std (100th)_y': 'std (100th)_tf10',\n",
    "    'mean (50th)_x': 'mean (50th)_tf8',\n",
    "    'mean (50th)_y': 'mean (50th)_tf10',\n",
    "    'std (50th)_x': 'std (50th)_tf8',\n",
    "    'std (50th)_y': 'std (50th)_tf10',\n",
    "    'mean (80th)_x': 'mean (80th)_tf8',\n",
    "    'mean (80th)_y': 'mean (80th)_tf10',\n",
    "    'std (80th)_x': 'std (80th)_tf8',\n",
    "    'std (80th)_y': 'std (80th)_tf10'\n",
    "\n",
    "})\n",
    "\n",
    "best_ant_hyper = pd.read_csv('tuning_results/tune_23/result/tuning_result_ant_lengthscale1.0_sampling_lr0.001_delta0.25.csv')\n",
    "best_ant_hyper = best_ant_hyper[best_ant_hyper['mean (100th)']>0.96] \n",
    "best_dkitty_hyper = pd.read_csv('tuning_results/tune_23/result/tuning_result_dkitty_lengthscale1.0_sampling_lr0.001_delta0.25.csv')\n",
    "best_dkitty_hyper = best_dkitty_hyper[best_dkitty_hyper['mean (100th)']>0.97]\n",
    "best_ant_hyper = best_ant_hyper.merge(best_dkitty_hyper,on = ['sampling_lr','eta','alpha','delta','lengthscale','classifier_free_guidance_weight']) \n",
    "\n",
    "best_ant_hyper = best_ant_hyper[['eta','alpha','classifier_free_guidance_weight','mean (100th)_x','mean (100th)_y','std (100th)_x','std (100th)_y','mean (50th)_x','mean (50th)_y','std (50th)_x','std (50th)_y','mean (80th)_x','mean (80th)_y','std (80th)_x','std (80th)_y']]\n",
    "\n",
    "best_ant_hyper = best_ant_hyper.rename(columns={\n",
    "    'eta': 'eta_cont',\n",
    "    'alpha': 'alpha',\n",
    "    'classifier_free_guidance_weight': 'classifier_free_guidance_weight',\n",
    "    'mean (100th)_x': 'mean (100th)_ant',\n",
    "    'mean (100th)_y': 'mean (100th)_dkitty',\n",
    "    'std (100th)_x': 'std (100th)_ant',\n",
    "    'std (100th)_y': 'std (100th)_dkitty',\n",
    "    'mean (50th)_x': 'mean (50th)_ant',\n",
    "    'mean (50th)_y': 'mean (50th)_dkitty',\n",
    "    'std (50th)_x': 'std (50th)_ant',\n",
    "    'std (50th)_y': 'std (50th)_dkitty',\n",
    "    'mean (80th)_x': 'mean (80th)_ant',\n",
    "    'mean (80th)_y': 'mean (80th)_dkitty',\n",
    "    'std (80th)_x': 'std (80th)_ant',\n",
    "    'std (80th)_y': 'std (80th)_dkitty'\n",
    "})\n",
    "best_ant_hyper = best_ant_hyper.merge(best_tf8_hyper, on=['alpha','classifier_free_guidance_weight'])\n",
    "best_ant_hyper[['num_fit_samples','eta_tf','eta_cont','alpha','classifier_free_guidance_weight','mean (100th)_ant','std (100th)_ant','mean (100th)_dkitty','std (100th)_dkitty','mean (100th)_tf8','std (100th)_tf8', 'mean (100th)_tf10', 'std (100th)_tf10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_fit_samples</th>\n",
       "      <th>eta_tf</th>\n",
       "      <th>eta_cont</th>\n",
       "      <th>alpha</th>\n",
       "      <th>classifier_free_guidance_weight</th>\n",
       "      <th>mean (50th)_ant</th>\n",
       "      <th>std (50th)_ant</th>\n",
       "      <th>mean (50th)_dkitty</th>\n",
       "      <th>std (50th)_dkitty</th>\n",
       "      <th>mean (50th)_tf8</th>\n",
       "      <th>std (50th)_tf8</th>\n",
       "      <th>mean (50th)_tf10</th>\n",
       "      <th>std (50th)_tf10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.716448</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>0.921065</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.674995</td>\n",
       "      <td>0.026171</td>\n",
       "      <td>0.472681</td>\n",
       "      <td>0.004093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.720881</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.920798</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.674995</td>\n",
       "      <td>0.026171</td>\n",
       "      <td>0.472681</td>\n",
       "      <td>0.004093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.711568</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.919340</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.674995</td>\n",
       "      <td>0.026171</td>\n",
       "      <td>0.472681</td>\n",
       "      <td>0.004093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_fit_samples  eta_tf  eta_cont  alpha  classifier_free_guidance_weight  \\\n",
       "0            15000     0.2      0.00    0.8                             -1.5   \n",
       "1            15000     0.2      0.05    0.8                             -1.5   \n",
       "2            15000     0.2      0.20    0.8                             -1.5   \n",
       "\n",
       "   mean (50th)_ant  std (50th)_ant  mean (50th)_dkitty  std (50th)_dkitty  \\\n",
       "0         0.716448        0.020384            0.921065           0.002660   \n",
       "1         0.720881        0.013926            0.920798           0.002807   \n",
       "2         0.711568        0.013848            0.919340           0.002913   \n",
       "\n",
       "   mean (50th)_tf8  std (50th)_tf8  mean (50th)_tf10  std (50th)_tf10  \n",
       "0         0.674995        0.026171          0.472681         0.004093  \n",
       "1         0.674995        0.026171          0.472681         0.004093  \n",
       "2         0.674995        0.026171          0.472681         0.004093  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ant_hyper[['num_fit_samples','eta_tf','eta_cont','alpha','classifier_free_guidance_weight','mean (50th)_ant','std (50th)_ant','mean (50th)_dkitty','std (50th)_dkitty','mean (50th)_tf8','std (50th)_tf8', 'mean (50th)_tf10', 'std (50th)_tf10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_fit_samples</th>\n",
       "      <th>eta_tf</th>\n",
       "      <th>eta_cont</th>\n",
       "      <th>alpha</th>\n",
       "      <th>classifier_free_guidance_weight</th>\n",
       "      <th>mean (80th)_ant</th>\n",
       "      <th>std (80th)_ant</th>\n",
       "      <th>mean (80th)_dkitty</th>\n",
       "      <th>std (80th)_dkitty</th>\n",
       "      <th>mean (80th)_tf8</th>\n",
       "      <th>std (80th)_tf8</th>\n",
       "      <th>mean (80th)_tf10</th>\n",
       "      <th>std (80th)_tf10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.839375</td>\n",
       "      <td>0.010242</td>\n",
       "      <td>0.940125</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.83926</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.525875</td>\n",
       "      <td>0.00717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.841409</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.939602</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.83926</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.525875</td>\n",
       "      <td>0.00717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.847239</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.938429</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.83926</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.525875</td>\n",
       "      <td>0.00717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_fit_samples  eta_tf  eta_cont  alpha  classifier_free_guidance_weight  \\\n",
       "0            15000     0.2      0.00    0.8                             -1.5   \n",
       "1            15000     0.2      0.05    0.8                             -1.5   \n",
       "2            15000     0.2      0.20    0.8                             -1.5   \n",
       "\n",
       "   mean (80th)_ant  std (80th)_ant  mean (80th)_dkitty  std (80th)_dkitty  \\\n",
       "0         0.839375        0.010242            0.940125           0.001418   \n",
       "1         0.841409        0.010893            0.939602           0.001767   \n",
       "2         0.847239        0.005095            0.938429           0.001829   \n",
       "\n",
       "   mean (80th)_tf8  std (80th)_tf8  mean (80th)_tf10  std (80th)_tf10  \n",
       "0          0.83926        0.014803          0.525875          0.00717  \n",
       "1          0.83926        0.014803          0.525875          0.00717  \n",
       "2          0.83926        0.014803          0.525875          0.00717  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ant_hyper[['num_fit_samples','eta_tf','eta_cont','alpha','classifier_free_guidance_weight','mean (80th)_ant','std (80th)_ant','mean (80th)_dkitty','std (80th)_dkitty','mean (80th)_tf8','std (80th)_tf8', 'mean (80th)_tf10', 'std (80th)_tf10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "best_ant_hyper = pd.read_csv('tuning_results/tune_23/result/tuning_result_ant_lengthscale1.0_sampling_lr0.001_delta0.25.csv')\n",
    "best_ant_hyper = best_ant_hyper[best_ant_hyper['mean (100th)']>0.96]\n",
    "#best_ant_hyper = best_ant_hyper[best_ant_hyper['alpha']==0.7]\n",
    "#best_ant_hyper = best_ant_hyper[best_ant_hyper['eta']==0.05]\n",
    "best_dkitty_hyper = pd.read_csv('tuning_results/tune_23/result/tuning_result_dkitty_lengthscale1.0_sampling_lr0.001_delta0.25.csv')\n",
    "best_dkitty_hyper = best_dkitty_hyper[best_dkitty_hyper['mean (100th)']>0.96]\n",
    "best_dkitty_hyper.merge(best_ant_hyper,on=['sampling_lr','lengthscale','delta','eta','alpha','classifier_free_guidance_weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n='abc'\n",
    "x = f'{n}12'\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
